<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Intent3D ICLR2025">
  <title>Intent3D ICLR2025</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention</h1>
          <h1 class="title is-1 publication-title"> <font color="Chestnut Red">ICLR 2025, Singapore</font> </h1> <br>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://weitaikang.github.io/">Weitai Kang<sup>1</sup></a>,</span>
            <span class="author-block">
              <a href="https://github.com/qumengxue">Mengxue Qu<sup>2</sup></a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=nDEy1QsAAAAJ&hl=en">Jyoti Kini<sup>3</sup></a>,</span>
            <span class="author-block">
              <a href="https://weiyc.github.io/">Yunchao Wei<sup>2</sup></a>,</span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah<sup>3</sup></a>,</span>
            <span class="author-block">
              <a href="https://tomyan555.github.io/">Yan Yan<sup>1</sup></a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Illinois at Chicago, <sup>2</sup>Beijing Jiaotong University, <sup>3</sup>University of Central Florida</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.18295"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/WeitaiKang/Intent3D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://weitaikang.github.io/Intent3D-webpage/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://weitaikang.github.io/Intent3D-webpage/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-image"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              
              
              
            </div>
            


            

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="motivation">
  <div class="container is-max-desktop">
     
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        
        <img src="images/motive.png" style="width: 100%;">

        <p style="text-align: justify; margin-top: 20px;">
          Motivation: We introduce 3D intention grounding (right), a new task for detecting the target object using a 3D bounding box in a 3D scene, guided by human intention sentence (e.g., <em>"I want something to support my back to relieve the pressure"</em>). In contrast, the existing 3D visual grounding (left) relies on human reasoning and references for detection. The illustration clearly distinguishes that observation and reasoning are manually executed by human (left) and automated by AI (right).
        </p>
    </div>
    </div> 
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Humans seek out objects in the 3D world to fulfill their daily needs or intentions. This inspires us to introduce 3D intention grounding, a new task in 3D object detection employing RGB-D, based on human intention, such as <em>"I want something to support my back."</em> Closely related, 3D visual grounding focuses on understanding human reference. To achieve detection based on human intention, it relies on humans to observe the scene, reason out the target that aligns with their intention (<em>"pillow"</em> in this case), and finally provide a reference to AI, such as <em>"A pillow on the couch"</em>. Instead, 3D intention grounding challenges AI agents to automatically observe, reason and detect the desired target solely based on human intention. To tackle this challenge, we introduce the new <strong>Intent3D</strong> dataset, consisting of 44,990 intention texts associated with 209 fine-grained classes from 1,042 scenes of the ScanNet dataset. We also establish several baselines based on different language-based 3D object detection models on our benchmark. Finally, we propose <strong>IntentNet</strong>, our unique approach, designed to tackle this intention-based detection problem. It focuses on three key aspects: intention understanding, reasoning to identify object candidates, and cascaded adaptive learning that leverages the intrinsic priority logic of different losses for multiple objective optimization. 
          </p>
        </div>
      </div>
    </div>
</section>


<section class="section" id="Method">
  <div class="container is-max-desktop">
     
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        
        <img src="images/data.png" style="width: 100%;">

          <p style="text-align: justify; margin-top: 20px;">
            Our benchmark dataset, <strong>Intent3D</strong>, is built through a structured pipeline: (1) Scene Graph Construction, where we organize scene information, including object categories and bounding boxes; (2) Object Selection, ensuring diverse, non-trivial, and unambiguous objects; (3) Text Generation, leveraging GPT-4 to produce intention descriptions without explicit object mentions, encouraging reasoning-based grounding; and (4) Data Cleaning, where we manually refine the generated texts to enhance quality and clarity. Intent3D provides a rich and challenging testbed for studying intention-driven object grounding in 3D environments.
          </p>
    </div>
    </div> 
    <!-- <br> -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
          <img src="images/method.png" style="width: 100%;">

          <p style="text-align: justify; margin-top: 20px;">
            We propose <strong>IntentNet</strong> to address 3D-IG, which requires 3D perception, intention understanding, and joint supervision. Our model extracts multimodal features using PointNet++, RoBERTa, and GroupFree for point cloud, text, and 3D object detection. An attention-based encoder fuses these features, followed by a decoder that refines queries for intention comprehension and box prediction. To enhance 3D understanding, we introduce Candidate Box Matching, aligning detected objects with intent. Verb-Object Alignment ensures the model captures intention semantics through contrastive learning. Finally, Cascaded Adaptive Learning optimizes training by structuring loss functions in a logical sequence, improving multimodal reasoning and performance.
    </div>
    </div> 

</section>


<section class="section" id="motivation">
  <div class="container is-max-desktop">
     
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <img src="images/exp.png" style="width: 100%;">

        <p style="text-align: justify; margin-top: 20px;">
          Our IntentNet achieves SOTA performance on Intent3D, outperforming prior methods by explicitly modeling intention language comprehension and reasoning over candidate boxes with cascaded optimization. On the validation set, it improves Top1-Acc@0.25 and Top1-Acc@0.5 by 11.22% and 8.05%, respectively, while boosting AP@0.25 and AP@0.5 by 9.12% and 5.43%. Similar gains are observed on the test set. Expert models, originally designed for referential language, struggle with intention language since they primarily align with nouns rather than verb-object relations, leading to inferior performance. Foundation models like 3D-VisTA benefit from broad multimodal pretraining but fall short due to their reliance on imperfect detector outputs, whereas IntentNet performs reasoning over candidate boxes, achieving better results despite using a less powerful detector. LLM-based models, such as Chat-3D-v2, perform the worst, as LLMs generally struggle with 3D-VG, and 3D-IG is even more complex. Their hallucination issues significantly impact AP metrics, though their pretrained detector and strong categorical reasoning help maintain decent Top1-Acc.
        </p>
    </div>
    </div>
    
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{kang2024intent3d,
        title={Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention},
        author={Kang, Weitai and Qu, Mengxue and Kini, Jyoti and Wei, Yunchao and Shah, Mubarak and Yan, Yan},
        journal={arXiv preprint arXiv:2405.18295},
        year={2024}
      }
    </code></pre>
  </div>
</section>
</body>
</html>



</body>
</html>
