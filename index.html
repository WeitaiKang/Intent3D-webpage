<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Intent3D ICLR2025">
  <title>Intent3D ICLR2025</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention</h1>
          <h1 class="title is-1 publication-title"> <font color="Chestnut Red">ICLR 2025, Singapore</font> </h1> <br>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://weitaikang.github.io/">Weitai Kang</a>,</span>
            <span class="author-block">
              <a href="https://github.com/qumengxue">Mengxue Qu</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=nDEy1QsAAAAJ&hl=en">Jyoti Kini</a>,</span>
            <span class="author-block">
              <a href="https://weiyc.github.io/">Yunchao Wei</a>,</span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>,</span>
            <span class="author-block">
              <a href="https://tomyan555.github.io/">Yan Yan</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Computer Vision and Multimedia Laboratory (CVM Lab)</span> <br>
            <span class="author-block">University of Illinois at Chicago</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.18295"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/WeitaiKang/Intent3D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=bWOd8_JpjQs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <span class="link-block">
                <a href="static/images/finepseudo_eccv24_poster_v2-1.png"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-image"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              
              
              
            </div>
            


            

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="motivation">
  <div class="container is-max-desktop">
     
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

        <embed src="images/fig1.pdf" type="application/pdf" width="100%">

        <p style="text-align: justify; margin-top: 20px;">
          Motivation: We introduce 3D intention grounding (right), a new task for detecting the object of interest using a 3D bounding box in a 3D scene, guided by human intention expressed in text (e.g., <em>"I want something to support my back to relieve the pressure"</em>). In contrast, the existing 3D visual grounding (left) relies on human reasoning and references for detection. The illustration clearly distinguishes that observation and reasoning are manually executed by human (left) and automated by AI (right).
        </p>
    </div>
    </div> 
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In real-life scenarios, humans seek out objects in the 3D world to fulfill their daily needs or intentions. This inspires us to introduce 3D intention grounding, a new task in 3D object detection employing RGB-D, based on human intention, such as <em>"I want something to support my back."</em> Closely related, 3D visual grounding focuses on understanding human reference. To achieve detection based on human intention, it relies on humans to observe the scene, reason out the target that aligns with their intention (<em>"pillow"</em> in this case), and finally provide a reference to the AI system, such as <em>"A pillow on the couch"</em>. Instead, 3D intention grounding challenges AI agents to automatically observe, reason and detect the desired target solely based on human intention. To tackle this challenge, we introduce the new <strong>Intent3D</strong> dataset, consisting of 44,990 intention texts associated with 209 fine-grained classes from 1,042 scenes of the ScanNet dataset. We also establish several baselines based on different language-based 3D object detection models on our benchmark. Finally, we propose <strong>IntentNet</strong>, our unique approach, designed to tackle this intention-based detection problem. It focuses on three key aspects: intention understanding, reasoning to identify object candidates, and cascaded adaptive learning that leverages the intrinsic priority logic of different losses for multiple objective optimization. 
          </p>
        </div>
      </div>
    </div>
</section>


<section class="section" id="Method">
  <div class="container is-max-desktop">
     
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        
          <embed src="images/data_collect.pdf" type="application/pdf" width="100%">

          <p style="text-align: justify; margin-top: 20px;">
            <strong>Alignability-Verification based Metric Learning</strong> is proposed to decide how well two video instances are alignable and produce an `alignability score' for effective learning from a limited labeled set. Our approach employs a triplet loss , considering videos from identical action classes as positive and those from different classes as negative. We selectively mine hard-negatives from the sampled minibatch based on alignment distance, presenting a challenging learning task for the model. Additionally, we incorporate a matching loss to quantify the alignment between videos, serving as a verification task to determine whether a video pair belongs to the same class (i.e., alignable or target label = 1) or different classes (i.e., non-alignable or target label = 0).
          </p>
    </div>
    </div> 
    <!-- <br> -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
          <img src="static/images/colab_pl_dia_eccv2.drawio-1.png" style="width: 100%;">

          <p style="text-align: justify; margin-top: 20px;">
            <strong>Collaborative Pseudo-labeling:</strong> The unlabeled instance undergoes processing by both video encoders (Action Encoder and Alignability Encoder). For the Action Encoder, its prediction is derived via its classification head. For the Alignability Encoder, the embedding of the instance computes class-wise alignability scores against a gallery of labeled embeddings. These scores are then used to generate a class-wise prediction using the non-parametric classifier. As these predictions stem from distinct supervisory signals—one from video-level and the other from alignability-based supervision—they offer complementary insights, resulting in a refined collaborative pseudo-label.
          </p>
    </div>
    </div> 

</section>


<section class="section" id="motivation">
  <div class="container is-max-desktop">
     
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <img src="static/images/fg_table.png" style="width: 100%;">

        <p style="text-align: justify; margin-top: 20px;">
          <strong>Comparison with state-of-the-art semi-supervised methods on Fine-grained Action recognition datasets</strong> under various % of labeled data settings. Highlighted <span style="color:red; font-weight:bold;">Red</span> shows the best results and <span style="color:blue; text-decoration:underline;">Blue</span> shows second best results. All results are reported on R2plus1D-18 utilizing the exact same amount of training data.
        </p>
    </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <!-- <h2 class="title is-3">Results</h2> -->
      <img src="static/images/coarsegrained_table.png" style="width: 100%;">

      <p style="text-align: justify; margin-top: 20px;">
        <strong>Results on standard Coarse-grained Action recognition datasets</strong> at various % of labeled set. Highlighted <span style="color:red; font-weight:bold;">Red</span> shows the best and <span style="color:blue; text-decoration:underline;">Blue</span> shows second best results.

      </p>
  </div>
    <!-- </div>  -->
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{dave2024finepseudo,
        title={FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition},
        author={Dave, Ishan and Rizve, Mamshad Nayeem and Shah, Mubarak},
        booktitle={European Conference on Computer Vision (ECCV)},
        year={2024}
      }
    </code></pre>
  </div>
</section>
</body>
</html>



</body>
</html>
