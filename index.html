<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Intent3D ICLR2025">
  <title>Intent3D ICLR2025</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention</h1>
          <h1 class="title is-1 publication-title"> <font color="Chestnut Red">ICLR 2025, Singapore</font> </h1> <br>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://weitaikang.github.io/">Weitai Kang</a>,</span>
            <span class="author-block">
              Mengxue Qu,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=nDEy1QsAAAAJ&hl=en">Jyoti Kini</a>,</span>
            <span class="author-block">
              <a href="https://weiyc.github.io/">Yunchao Wei</a>,</span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>,</span>
            <span class="author-block">
              <a href="https://tomyan555.github.io/">Yan Yan</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Computer Vision and Multimedia Laboratory (CVM Lab)</span> <br>
            <span class="author-block">University of Illinois at Chicago</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2409.01448"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/DAVEISHAN/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=bWOd8_JpjQs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <span class="link-block">
                <a href="static/images/finepseudo_eccv24_poster_v2-1.png"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-image"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              
              
              
            </div>
            


            

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="motivation">
  <div class="container is-max-desktop">
     
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

        <img src="static/images/fgteaser_try14.drawio-1.png" style="width: 100%;">

        <p style="text-align: justify; margin-top: 20px;">
          <strong>Motivation: (a)</strong> Samples actions from the standard coarse-grained action recognition dataset (UCF101) <strong>(b)</strong> Sample actions from fine-grained action recognition dataset (Diving48) <strong>(c)</strong> For proof-of-concept, we choose a binary classification problem of fine-grained actions, where the model has to predict whether the pair of videos belong to the same class or not. We consider Diving48 dataset with 10% training data. We first obtain frame-wise video embedding from a pretrained framewise video-encoder model. The top part of (c) shows that cosine distance computed at each timestamp does not provide a discriminative measure, whereas, DTW-based alignment cost provides a clear difference in pair of same vs different classes. The bottom part of (c), shows the performance of the binary classification task in terms of average precision, where our alignability-score significantly outperforms the other standard distances. 
        </p>
    </div>
    </div> 
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Real-life applications of action recognition often require a fine-grained understanding of subtle movements, e.g., in sports analytics, user interactions in AR/VR, and surgical videos. Although fine-grained actions are more costly to annotate, existing semi-supervised action recognition has mainly focused on coarse-grained action recognition. Since fine-grained actions are more challenging due to the absence of scene bias, classifying these actions requires an understanding of action-phases. Hence, existing coarse-grained semi-supervised methods do not work effectively. In this work, we for the first time thoroughly investigate semi-supervised fine-grained action recognition (FGAR). We observe that alignment distances like dynamic time warping (DTW) provide a suitable action-phase-aware measure for comparing fine-grained actions, a concept previously unexploited in FGAR. However, since regular DTW distance is pairwise and assumes strict alignment between pairs, it is not directly suitable for classifying fine-grained actions. To utilize such alignment distances in a limited-label setting, we propose an Alignability-Verification-based Metric learning technique to effectively discriminate between fine-grained action pairs. Our learnable alignability score provides a better phase-aware measure, which we use to refine the pseudo-labels of the primary video encoder. Our collaborative pseudo-labeling-based framework 'FinePseudo' significantly outperforms prior methods on four fine-grained action recognition datasets: Diving48, FineGym99, FineGym288, and FineDiving, and shows improvement on existing coarse-grained datasets: Kinetics400 and Something-SomethingV2. We also demonstrate the robustness of our collaborative pseudo-labeling in handling novel unlabeled classes in open-world semi-supervised setups.
          </p>
        </div>
      </div>
    </div>
</section>


<section class="section" id="Method">
  <div class="container is-max-desktop">
     
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
          <img src="static/images/alignability_verification_dia_eccv1.drawio-1.png" style="width: 100%;">

          <p style="text-align: justify; margin-top: 20px;">
            <strong>Alignability-Verification based Metric Learning</strong> is proposed to decide how well two video instances are alignable and produce an `alignability score' for effective learning from a limited labeled set. Our approach employs a triplet loss , considering videos from identical action classes as positive and those from different classes as negative. We selectively mine hard-negatives from the sampled minibatch based on alignment distance, presenting a challenging learning task for the model. Additionally, we incorporate a matching loss to quantify the alignment between videos, serving as a verification task to determine whether a video pair belongs to the same class (i.e., alignable or target label = 1) or different classes (i.e., non-alignable or target label = 0).
          </p>
    </div>
    </div> 
    <!-- <br> -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
          <img src="static/images/colab_pl_dia_eccv2.drawio-1.png" style="width: 100%;">

          <p style="text-align: justify; margin-top: 20px;">
            <strong>Collaborative Pseudo-labeling:</strong> The unlabeled instance undergoes processing by both video encoders (Action Encoder and Alignability Encoder). For the Action Encoder, its prediction is derived via its classification head. For the Alignability Encoder, the embedding of the instance computes class-wise alignability scores against a gallery of labeled embeddings. These scores are then used to generate a class-wise prediction using the non-parametric classifier. As these predictions stem from distinct supervisory signals—one from video-level and the other from alignability-based supervision—they offer complementary insights, resulting in a refined collaborative pseudo-label.
          </p>
    </div>
    </div> 

</section>


<section class="section" id="motivation">
  <div class="container is-max-desktop">
     
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <img src="static/images/fg_table.png" style="width: 100%;">

        <p style="text-align: justify; margin-top: 20px;">
          <strong>Comparison with state-of-the-art semi-supervised methods on Fine-grained Action recognition datasets</strong> under various % of labeled data settings. Highlighted <span style="color:red; font-weight:bold;">Red</span> shows the best results and <span style="color:blue; text-decoration:underline;">Blue</span> shows second best results. All results are reported on R2plus1D-18 utilizing the exact same amount of training data.
        </p>
    </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <!-- <h2 class="title is-3">Results</h2> -->
      <img src="static/images/coarsegrained_table.png" style="width: 100%;">

      <p style="text-align: justify; margin-top: 20px;">
        <strong>Results on standard Coarse-grained Action recognition datasets</strong> at various % of labeled set. Highlighted <span style="color:red; font-weight:bold;">Red</span> shows the best and <span style="color:blue; text-decoration:underline;">Blue</span> shows second best results.

      </p>
  </div>
    <!-- </div>  -->
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{dave2024finepseudo,
        title={FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition},
        author={Dave, Ishan and Rizve, Mamshad Nayeem and Shah, Mubarak},
        booktitle={European Conference on Computer Vision (ECCV)},
        year={2024}
      }
    </code></pre>
  </div>
</section>
</body>
</html>



</body>
</html>
